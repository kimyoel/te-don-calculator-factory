"""
run_agent.py

ê°„ë‹¨í•œ Python ë£¨í”„ ì—ì´ì „íŠ¸:
 - MCP ì„œë²„(mcp_server.py)ë¥¼ ì‹¤í–‰í•´ë‘” ìƒíƒœì—ì„œ, MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ íˆ´ì„ í˜¸ì¶œ
 - í•˜ë£¨ ëª©í‘œëŸ‰(TARGET_PER_DAY)ì„ ë§ì¶œ ë•Œê¹Œì§€, ë¶€ì¡±ë¶„ì€ planner_suggest_cases + append_casesë¡œ ì¶©ì›
 - run_case_pipelineìœ¼ë¡œ ì¼€ì´ìŠ¤ë¥¼ ì²˜ë¦¬í•´ publishedë¥¼ ì˜¬ë¦°ë‹¤.

ì‹¤í–‰ ì „ì œ:
 - ë³„ë„ í„°ë¯¸ë„ì—ì„œ `python mcp_server.py` ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨
"""

from __future__ import annotations

import argparse
import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

from fastmcp import Client

SERVER_URL = "http://127.0.0.1:8765/mcp"
DEFAULT_CONFIG_PATH = Path("config.json")
DEFAULTS = {
    "target_per_day": 10,
    "max_refill_loops": 3,
    "domain_type": "debt",
    "openai_model_writer": "gpt-5-mini",
    "openai_model_safety": "gpt-5-mini",
    "similarity_threshold": 0.4,
    "initial_launch_limit": 100,
}


def _unwrap(result) -> Any:
    """CallToolResultë‚˜ Pydantic ê°ì²´ë¥¼ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ í‰íƒ„í™”."""

    def normalize(obj):
        if obj is None:
            return None
        # Pydantic v2 ëª¨ë¸ ì²˜ë¦¬
        if hasattr(obj, "model_dump"):
            return normalize(obj.model_dump())
        # Pydantic v1 í˜¸í™˜
        if hasattr(obj, "dict"):
            return normalize(obj.dict())
        # Pydantic RootModel(v1) ë˜ëŠ” root í•„ë“œ í˜¸í™˜
        if hasattr(obj, "__root__"):
            try:
                return normalize(obj.__root__)  # type: ignore[attr-defined]
            except Exception:
                pass
        if hasattr(obj, "root"):
            try:
                return normalize(obj.root)  # type: ignore[attr-defined]
            except Exception:
                pass
        # ì¼ë°˜ ê°ì²´ë¥¼ dictë¡œ ë³€í™˜ ì‹œë„
        if hasattr(obj, "__dict__"):
            return {k: normalize(v) for k, v in obj.__dict__.items()}
        if isinstance(obj, dict):
            return {k: normalize(v) for k, v in obj.items()}
        if isinstance(obj, (list, tuple, set)):
            return [normalize(v) for v in obj]
        return obj

    if result is None:
        return None
    # 1) structured_contentì— result í‚¤ê°€ ìˆìœ¼ë©´ ìµœìš°ì„ 
    structured = getattr(result, "structured_content", None)
    if structured not in (None, {}, []):
        if isinstance(structured, dict) and "result" in structured:
            return normalize(structured["result"])
        return normalize(structured)
    # 2) CallToolResult.data
    if getattr(result, "data", None) is not None:
        return normalize(result.data)
    # content fallback
    if getattr(result, "content", None) is not None:
        return normalize(result.content)
    return normalize(result)


def count_published_today(client: Client) -> int:
    # ê°„ë‹¨íˆ ì „ì²´ publishedë¥¼ ì„¸ëŠ” íˆ´ì´ ì—†ìœ¼ë¯€ë¡œ todo ê¸°ë°˜ìœ¼ë¡œë§Œ ì§„í–‰í•˜ê³ ,
    # ì‹¤ì œ published ì¹´ìš´íŠ¸ëŠ” íŒŒì´í”„ë¼ì¸ ê²°ê³¼ì— ë”°ë¼ ì¦ê°€ì‹œí‚¨ë‹¤.
    # ì´ˆê¸° published_todayëŠ” 0ìœ¼ë¡œ ì‹œì‘í•´ ë£¨í”„ ë‚´ì—ì„œ ëˆ„ì .
    return 0


async def list_todo(client: Client, limit: int) -> List[Dict[str, Any]]:
    res = await client.call_tool("list_todo_cases", {"limit": limit})
    logging.debug("raw list_todo_cases result: %r", res)
    data = _unwrap(res) or []
    if not isinstance(data, list):
        return []
    # dictë¡œ ê°•ì œ ë³€í™˜ ë° í•„í„°ë§
    cleaned: List[Dict[str, Any]] = []
    for item in data:
        if hasattr(item, "model_dump"):
            item = item.model_dump()
        if hasattr(item, "__dict__") and not isinstance(item, dict):
            item = dict(item.__dict__)
        if not isinstance(item, dict):
            continue
        if not item.get("case_id"):
            continue
        cleaned.append(item)
    return cleaned


async def plan_and_append(client: Client, product_type: str, shortage: int, dry_run: bool) -> int:
    ideas = _unwrap(
        await client.call_tool(
            "planner_suggest_cases",
            {
                "product_type": product_type,
                "max_n": shortage * 2 if shortage > 0 else 2,
            },
        )
    )
    if not ideas:
        return 0
    if dry_run:
        logging.info("ğŸ§ª DRY-RUN: plannerê°€ %sê±´ ë§Œë“¤ì—ˆì§€ë§Œ DBì—ëŠ” ì•ˆ ë„£ì–´ìš”.", len(ideas))
        return len(ideas)
    await client.call_tool("append_cases", {"cases": ideas})
    return len(ideas)


async def process_todo(
    client: Client,
    todo: List[Dict[str, Any]],
    needed: int,
    dry_run: bool,
    published_ids: List[str],
    discarded: List[str],
) -> int:
    published = 0
    for row in todo:
        if published >= needed:
            break
        case_id = row.get("case_id") or row.get("id")
        if not case_id:
            continue
        if dry_run:
            logging.info("ğŸ§ª DRY-RUN: run_case_pipeline íŒ¨ìŠ¤ (%s)", case_id)
            continue
        result = _unwrap(
            await client.call_tool(
                "run_case_pipeline",
                {"case_id": case_id, "max_attempts": 2},
            )
        ) or {}
        status = result.get("status")
        if status == "published":
            published += 1
            published_ids.append(case_id)
            logging.info("âœ… ë°œí–‰ ì™„ë£Œ: %s -> %s", case_id, result.get("html_path"))
        elif status == "discarded":
            discarded.append(f"{case_id}:{result}")
            logging.info("ğŸ§¹ íê¸°ë¨: %s (%s)", case_id, result)
        else:
            logging.info("â„¹ï¸ ì²˜ë¦¬ ê²°ê³¼: %s (%s)", case_id, result)
    return published


def load_config(path: Path) -> Dict[str, Any]:
    if not path.exists():
        logging.warning("âš ï¸ config íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: %s", path)
        return dict(DEFAULTS)
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        cfg = dict(DEFAULTS)
        cfg.update({k: v for k, v in data.items() if v is not None})
        return cfg
    except Exception as exc:  # noqa: BLE001
        logging.warning("âš ï¸ config ë¡œë“œ ì‹¤íŒ¨(%s), ê¸°ë³¸ê°’ ì‚¬ìš©: %s", path, exc)
        return dict(DEFAULTS)


def setup_logging(verbose: bool) -> Path:
    Path("logs").mkdir(exist_ok=True)
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    log_path = Path("logs") / f"run_agent-{ts}.log"
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s %(levelname)s: %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_path, encoding="utf-8"),
        ],
    )
    return log_path


async def main_async(args: argparse.Namespace) -> None:
    log_path = setup_logging(args.verbose)
    cfg = load_config(Path(args.config))
    target_per_day = int(cfg.get("target_per_day", DEFAULTS["target_per_day"]))
    max_refill_loops = int(cfg.get("max_refill_loops", DEFAULTS["max_refill_loops"]))
    domain_type = cfg.get("domain_type", DEFAULTS["domain_type"])
    initial_limit = int(cfg.get("initial_launch_limit", DEFAULTS["initial_launch_limit"]))

    logging.info("ğŸ”§ ì„¤ì • ë¡œë“œ: %s", cfg)
    logging.info("ğŸ—’ ë¡œê·¸ íŒŒì¼: %s", log_path)

    client = Client(SERVER_URL)
    published_ids: List[str] = []
    discarded: List[str] = []
    planned_total = 0

    try:
        async with client:
            try:
                test = await client.call_tool("list_todo_cases", {"limit": 1})
                unwrapped_test = _unwrap(test)
                logging.info("âœ… MCP ì—°ê²° ì„±ê³µ (%s)", SERVER_URL)
                logging.info("ğŸ—‚ list_todo_cases ì›ë³¸ ê²°ê³¼: %r", test)
                logging.info("ğŸ—‚ list_todo_cases í•´ì œ ê²°ê³¼: %s", unwrapped_test)
            except Exception as conn_exc:  # noqa: BLE001
                logging.error("âŒ MCP ì—°ê²° ì‹¤íŒ¨ (%s): %s", SERVER_URL, conn_exc)
                return

            # ì´ˆê¸° ë¡ ì¹­ ì•ˆì „ ëª¨ë“œ: ì „ì²´ published ìˆ˜ ìƒí•œ ì²´í¬
            from core import db  # ë¡œì»¬ ì„í¬íŠ¸ë¡œ ì˜ì¡´ ìµœì†Œí™”

            total_published = db.count_published_total()
            if initial_limit > 0 and (not args.ignore_initial_limit) and total_published >= initial_limit:
                logging.warning(
                    "âš ï¸ ì´ˆê¸° ë°œí–‰ ìƒí•œ ë„ë‹¬(%sê±´). SEO/ì½˜ì†” ë¦¬ë·°ê¹Œì§€ ìë™ ìƒì‚°ì„ ë©ˆì¶¥ë‹ˆë‹¤.",
                    total_published,
                )
                return

            published_today = count_published_today(client)
            loop_count = 0

            while published_today < target_per_day and loop_count < max_refill_loops:
                needed = target_per_day - published_today
                if needed <= 0:
                    break

                logging.info("ğŸ” ë£¨í”„ %s: í˜„ì¬ published=%s, í•„ìš”=%s", loop_count + 1, published_today, needed)

                todo = await list_todo(client, limit=needed * 2)
                if len(todo) < needed:
                    shortage = needed - len(todo)
                    logging.info("ğŸ“‹ TODO ë¶€ì¡± %sê°œ â†’ planner í˜¸ì¶œë¡œ ë³´ì¶©í•©ë‹ˆë‹¤.", shortage)
                    planned_total += await plan_and_append(
                        client,
                        product_type=domain_type,
                        shortage=shortage,
                        dry_run=args.dry_run,
                    )
                    todo = await list_todo(client, limit=needed * 2)

                gained = await process_todo(
                    client,
                    todo,
                    needed,
                    args.dry_run,
                    published_ids,
                    discarded,
                )
                published_today += gained
                loop_count += 1

            logging.info("ğŸ“Š ìš”ì•½: published=%s, discarded=%s, planned=%s, dry_run=%s",
                         len(published_ids), len(discarded), planned_total, args.dry_run)
            logging.info("ğŸŸ¢ ë°œí–‰ ID: %s", published_ids)
            logging.info("ğŸ§¹ íê¸° ëª©ë¡: %s", discarded)
            logging.info("ğŸ—’ ë¡œê·¸ íŒŒì¼: %s", log_path)
    except Exception as exc:  # noqa: BLE001
        logging.error("âŒ MCP í´ë¼ì´ì–¸íŠ¸ ì˜ˆì™¸: %s", exc)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.json", help="ì„¤ì • íŒŒì¼ ê²½ë¡œ (json)")
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ append/publish ì—†ì´ ë¡œê·¸ë§Œ ì¶œë ¥")
    parser.add_argument("--verbose", action="store_true", help="ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--ignore-initial-limit", action="store_true", help="initial_launch_limit ë¬´ì‹œ")
    args = parser.parse_args()
    asyncio.run(main_async(args))

